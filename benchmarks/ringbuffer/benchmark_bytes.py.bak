"""Benchmarks BytesRingBuffer performance with various message sizes.

Usage:
    uv run python benchmarks/ringbuffer/benchmark_bytes.py [--buffer-size SIZE]
    uv run python benchmarks/ringbuffer/benchmark_bytes.py --multi-size

Measures BytesRingBuffer operations:
- insert, insert_batch, consume, consume_all
- overwrite_latest, contains
- Async operations: aconsume, aconsume_iterable
- Comparison: disable_async=True vs False, only_insert_unique=True vs False
"""

from __future__ import annotations

import argparse
import asyncio
import gc
import time
from dataclasses import dataclass, field

import numpy as np

from mm_toolbox.ringbuffer.bytes import BytesRingBuffer


@dataclass
class BenchmarkConfig:
    """Configuration for benchmark run."""

    buffer_size: int
    num_operations: int = 100_000
    warmup_operations: int = 1_000
    item_sizes: list[int] = field(default_factory=lambda: [32, 128, 512, 2048])
    batch_sizes: list[int] = field(default_factory=lambda: [1, 10, 100, 1000])
    only_insert_unique: bool = False
    disable_async: bool = True


@dataclass
class OperationStats:
    """Statistics for a single operation type."""

    name: str
    latencies_ns: list[int] = field(default_factory=list)

    def add_latency(self, latency_ns: int) -> None:
        """Add a latency measurement."""
        self.latencies_ns.append(latency_ns)

    def compute_percentiles(self) -> dict[str, float]:
        """Compute percentile statistics."""
        if not self.latencies_ns:
            return {
                "count": 0,
                "mean": 0.0,
                "p10": 0.0,
                "p25": 0.0,
                "p50": 0.0,
                "p95": 0.0,
                "p99": 0.0,
                "p99_9": 0.0,
                "ops_per_sec": 0.0,
            }

        arr = np.array(self.latencies_ns, dtype=np.float64)
        mean_ns = float(np.mean(arr))
        ops_per_sec = 1e9 / mean_ns if mean_ns > 0 else 0

        return {
            "count": len(self.latencies_ns),
            "mean": mean_ns,
            "p10": float(np.percentile(arr, 10)),
            "p25": float(np.percentile(arr, 25)),
            "p50": float(np.percentile(arr, 50)),
            "p95": float(np.percentile(arr, 95)),
            "p99": float(np.percentile(arr, 99)),
            "p99_9": float(np.percentile(arr, 99.9)),
            "ops_per_sec": ops_per_sec,
        }


@dataclass
class BenchmarkResults:
    """Aggregated benchmark results."""

    config: BenchmarkConfig
    operations: dict[str, OperationStats] = field(default_factory=dict)

    def add_operation(self, name: str) -> OperationStats:
        """Add a new operation for tracking."""
        stats = OperationStats(name=name)
        self.operations[name] = stats
        return stats

    def get_operation(self, name: str) -> OperationStats:
        """Get existing operation stats."""
        return self.operations[name]

    def print_report(self) -> None:
        """Print formatted benchmark report."""
        async_mode = "disabled" if self.config.disable_async else "enabled"
        unique_mode = "enabled" if self.config.only_insert_unique else "disabled"
        print("=" * 100)
        print(f"BytesRingBuffer Benchmark Results (capacity={self.config.buffer_size}, async={async_mode}, unique={unique_mode})")
        print("=" * 100)
        print(f"Operations: {self.config.num_operations:,} (warmup: {self.config.warmup_operations:,})")
        print()

        print("Operation Performance")
        print("-" * 100)
        header = f"{'Operation':<30} {'Count':>8} {'Mean ns':>10} {'P50':>10} {'P95':>10} {'P99':>10} {'ops/sec':>12}"
        print(header)

        for name in sorted(self.operations.keys()):
            stats = self.operations[name]
            pcts = stats.compute_percentiles()
            if pcts["count"] == 0:
                continue

            print(
                f"{name:<30} {pcts['count']:>8} {pcts['mean']:>10.1f} "
                f"{pcts['p50']:>10.1f} {pcts['p95']:>10.1f} {pcts['p99']:>10.1f} "
                f"{pcts['ops_per_sec']:>12.0f}"
            )

        print("=" * 100)


class BytesRingBufferBenchmark:
    """Benchmark runner for BytesRingBuffer."""

    def __init__(self, config: BenchmarkConfig) -> None:
        """Initialize the benchmark.

        Args:
            config: Benchmark configuration.
        """
        self.config = config
        self.results = BenchmarkResults(config=config)
        self.data_by_size: dict[int, list[bytes]] = self._generate_data()

    def _generate_data(self) -> dict[int, list[bytes]]:
        """Generate synthetic bytes data for benchmarking.

        Returns:
            Dictionary mapping size to list of bytes objects.
        """
        np.random.seed(42)
        data_by_size = {}

        num_samples = self.config.num_operations + self.config.warmup_operations + 10000

        for size in self.config.item_sizes:
            data = []
            for _ in range(num_samples):
                random_bytes = np.random.bytes(size)
                data.append(random_bytes)
            data_by_size[size] = data

        return data_by_size

    def _benchmark_insert(self, rb: BytesRingBuffer, item_size: int) -> None:
        """Benchmark single insert operation."""
        stats = self.results.add_operation(f"insert(size={item_size})")
        data = self.data_by_size[item_size]

        for i in range(self.config.warmup_operations):
            rb.insert(data[i])

        for i in range(self.config.num_operations):
            idx = (i + self.config.warmup_operations) % len(data)
            start = time.perf_counter_ns()
            rb.insert(data[idx])
            elapsed = time.perf_counter_ns() - start
            stats.add_latency(elapsed)

    def _benchmark_insert_batch(self, rb: BytesRingBuffer, item_size: int) -> None:
        """Benchmark batch insert operations."""
        data = self.data_by_size[item_size]

        for batch_size in self.config.batch_sizes:
            stats = self.results.add_operation(f"insert_batch(size={item_size}, n={batch_size})")

            num_batches = self.config.num_operations // batch_size
            warmup_batches = self.config.warmup_operations // batch_size

            for batch_idx in range(warmup_batches):
                start_idx = (batch_idx * batch_size) % (len(data) - batch_size)
                batch = data[start_idx:start_idx + batch_size]
                rb.insert_batch(batch)

            for batch_idx in range(num_batches):
                start_idx = ((batch_idx + warmup_batches) * batch_size) % (len(data) - batch_size)
                batch = data[start_idx:start_idx + batch_size]

                start = time.perf_counter_ns()
                rb.insert_batch(batch)
                elapsed = time.perf_counter_ns() - start
                stats.add_latency(elapsed)

    def _benchmark_overwrite_latest(self, rb: BytesRingBuffer, item_size: int) -> None:
        """Benchmark overwrite_latest operation."""
        stats = self.results.add_operation(f"overwrite_latest(size={item_size})")
        data = self.data_by_size[item_size]

        rb.insert(data[0])

        for i in range(self.config.warmup_operations):
            rb.overwrite_latest(data[i])

        for i in range(self.config.num_operations):
            idx = (i + self.config.warmup_operations) % len(data)
            start = time.perf_counter_ns()
            rb.overwrite_latest(data[idx])
            elapsed = time.perf_counter_ns() - start
            stats.add_latency(elapsed)

    def _benchmark_consume(self, rb: BytesRingBuffer, item_size: int) -> None:
        """Benchmark single consume operation."""
        data = self.data_by_size[item_size]

        for i in range(self.config.warmup_operations + self.config.num_operations):
            rb.insert(data[i % len(data)])

        stats = self.results.add_operation(f"consume(size={item_size})")

        for _ in range(self.config.warmup_operations):
            if not rb.is_empty():
                rb.consume()

        for i in range(self.config.num_operations):
            if rb.is_empty():
                rb.insert(data[i % len(data)])

            start = time.perf_counter_ns()
            rb.consume()
            elapsed = time.perf_counter_ns() - start
            stats.add_latency(elapsed)

    def _benchmark_consume_all(self, rb: BytesRingBuffer, item_size: int) -> None:
        """Benchmark consume_all operation."""
        stats = self.results.add_operation(f"consume_all(size={item_size})")
        data = self.data_by_size[item_size]

        fill_size = min(self.config.buffer_size, 1000)
        num_iterations = self.config.num_operations // 100

        for _ in range(num_iterations):
            rb.insert_batch(data[:fill_size])

            start = time.perf_counter_ns()
            rb.consume_all()
            elapsed = time.perf_counter_ns() - start
            stats.add_latency(elapsed)

    def _benchmark_contains(self, rb: BytesRingBuffer, item_size: int) -> None:
        """Benchmark contains operation."""
        data = self.data_by_size[item_size]

        fill_size = min(self.config.buffer_size, 100)
        rb.insert_batch(data[:fill_size])

        stats = self.results.add_operation(f"contains(size={item_size})")

        for i in range(self.config.warmup_operations):
            rb.contains(data[i % len(data)])

        for i in range(self.config.num_operations):
            idx = i % len(data)
            start = time.perf_counter_ns()
            rb.contains(data[idx])
            elapsed = time.perf_counter_ns() - start
            stats.add_latency(elapsed)

    async def _benchmark_aconsume(self, rb: BytesRingBuffer, item_size: int) -> None:
        """Benchmark async consume operation."""
        data = self.data_by_size[item_size]

        for i in range(self.config.warmup_operations + self.config.num_operations):
            rb.insert(data[i % len(data)])

        stats = self.results.add_operation(f"aconsume(size={item_size})")

        for _ in range(self.config.warmup_operations):
            if not rb.is_empty():
                await rb.aconsume()

        for i in range(self.config.num_operations):
            if rb.is_empty():
                rb.insert(data[i % len(data)])

            start = time.perf_counter_ns()
            await rb.aconsume()
            elapsed = time.perf_counter_ns() - start
            stats.add_latency(elapsed)

    async def _benchmark_aconsume_iterable(self, rb: BytesRingBuffer, item_size: int) -> None:
        """Benchmark async consume iterable."""
        data = self.data_by_size[item_size]

        fill_size = min(self.config.buffer_size, 1000)

        stats = self.results.add_operation(f"aconsume_iterable(size={item_size})")

        num_iterations = self.config.num_operations // fill_size

        for iteration in range(num_iterations):
            rb.insert_batch(data[:fill_size])

            count = 0
            start = time.perf_counter_ns()
            async for _ in rb.aconsume_iterable():
                count += 1
                if count >= fill_size:
                    break
            elapsed = time.perf_counter_ns() - start

            if iteration >= (num_iterations // 10):
                stats.add_latency(elapsed // count if count > 0 else elapsed)

    def run_sync(self, item_size: int) -> None:
        """Run synchronous benchmarks for a given item size."""
        print(f"  - Testing insert (size={item_size})...")
        rb = BytesRingBuffer(
            max_capacity=self.config.buffer_size,
            disable_async=self.config.disable_async,
            only_insert_unique=self.config.only_insert_unique,
        )
        self._benchmark_insert(rb, item_size)

        print(f"  - Testing insert_batch (size={item_size})...")
        rb = BytesRingBuffer(
            max_capacity=self.config.buffer_size,
            disable_async=self.config.disable_async,
            only_insert_unique=self.config.only_insert_unique,
        )
        self._benchmark_insert_batch(rb, item_size)

        print(f"  - Testing overwrite_latest (size={item_size})...")
        rb = BytesRingBuffer(
            max_capacity=self.config.buffer_size,
            disable_async=self.config.disable_async,
            only_insert_unique=self.config.only_insert_unique,
        )
        self._benchmark_overwrite_latest(rb, item_size)

        print(f"  - Testing consume (size={item_size})...")
        rb = BytesRingBuffer(
            max_capacity=self.config.buffer_size,
            disable_async=self.config.disable_async,
            only_insert_unique=self.config.only_insert_unique,
        )
        self._benchmark_consume(rb, item_size)

        print(f"  - Testing consume_all (size={item_size})...")
        rb = BytesRingBuffer(
            max_capacity=self.config.buffer_size,
            disable_async=self.config.disable_async,
            only_insert_unique=self.config.only_insert_unique,
        )
        self._benchmark_consume_all(rb, item_size)

        print(f"  - Testing contains (size={item_size})...")
        rb = BytesRingBuffer(
            max_capacity=self.config.buffer_size,
            disable_async=self.config.disable_async,
            only_insert_unique=self.config.only_insert_unique,
        )
        self._benchmark_contains(rb, item_size)

    async def run_async(self, item_size: int) -> None:
        """Run asynchronous benchmarks for a given item size."""
        print(f"  - Testing aconsume (size={item_size})...")
        rb = BytesRingBuffer(
            max_capacity=self.config.buffer_size,
            disable_async=self.config.disable_async,
            only_insert_unique=self.config.only_insert_unique,
        )
        await self._benchmark_aconsume(rb, item_size)

        print(f"  - Testing aconsume_iterable (size={item_size})...")
        rb = BytesRingBuffer(
            max_capacity=self.config.buffer_size,
            disable_async=self.config.disable_async,
            only_insert_unique=self.config.only_insert_unique,
        )
        await self._benchmark_aconsume_iterable(rb, item_size)

    def run(self) -> None:
        """Run all benchmarks."""
        print(f"Starting BytesRingBuffer benchmark...")
        print(f"Buffer size: {self.config.buffer_size}")
        print(f"Async mode: {'disabled' if self.config.disable_async else 'enabled'}")
        print(f"Unique mode: {'enabled' if self.config.only_insert_unique else 'disabled'}")

        print(f"\nRunning synchronous benchmarks...")
        for item_size in self.config.item_sizes:
            self.run_sync(item_size)

        print(f"\nRunning asynchronous benchmarks...")
        for item_size in self.config.item_sizes:
            asyncio.run(self.run_async(item_size))

        print("\n" + "=" * 100)
        self.results.print_report()


def run_multi_size(base_config: BenchmarkConfig, disable_async: bool, only_insert_unique: bool) -> None:
    """Run benchmarks across multiple buffer sizes.

    Args:
        base_config: Base configuration to use.
        disable_async: Whether to disable async.
        only_insert_unique: Whether to enable deduplication.
    """
    sizes = [2**i for i in range(4, 13, 2)]

    async_mode = "disabled" if disable_async else "enabled"
    unique_mode = "enabled" if only_insert_unique else "disabled"
    print(f"\n{'=' * 100}")
    print(f"Multi-Size Benchmark (async={async_mode}, unique={unique_mode})")
    print(f"{'=' * 100}\n")

    all_results: list[tuple[int, BenchmarkResults]] = []

    for size in sizes:
        config = BenchmarkConfig(
            buffer_size=size,
            num_operations=base_config.num_operations,
            warmup_operations=base_config.warmup_operations,
            item_sizes=[128],
            batch_sizes=base_config.batch_sizes,
            disable_async=disable_async,
            only_insert_unique=only_insert_unique,
        )

        print(f"\n{'=' * 100}")
        print(f"Testing buffer size: {size}")
        print(f"{'=' * 100}")

        benchmark = BytesRingBufferBenchmark(config)
        benchmark.run()

        all_results.append((size, benchmark.results))

        gc.collect()

    print(f"\n{'=' * 100}")
    print(f"Comparative Summary (async={async_mode}, unique={unique_mode})")
    print(f"{'=' * 100}")

    print(f"{'Size':>8} {'insert':>12} {'consume':>12} {'contains':>12}")
    print("-" * 100)

    for size, results in all_results:
        insert_stats = results.operations.get("insert(size=128)", OperationStats("insert")).compute_percentiles()
        consume_stats = results.operations.get("consume(size=128)", OperationStats("consume")).compute_percentiles()
        contains_stats = results.operations.get("contains(size=128)", OperationStats("contains")).compute_percentiles()

        print(
            f"{size:>8} {insert_stats['mean']:>12.1f} {consume_stats['mean']:>12.1f} "
            f"{contains_stats['mean']:>12.1f}"
        )

    print("=" * 100)


def main() -> None:
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Benchmark BytesRingBuffer performance"
    )
    parser.add_argument(
        "--buffer-size",
        "-b",
        type=int,
        default=1024,
        help="Buffer capacity (default: 1024)",
    )
    parser.add_argument(
        "--operations",
        "-n",
        type=int,
        default=100_000,
        help="Number of operations to benchmark (default: 100,000)",
    )
    parser.add_argument(
        "--warmup",
        "-w",
        type=int,
        default=1_000,
        help="Number of warmup operations (default: 1,000)",
    )
    parser.add_argument(
        "--multi-size",
        "-m",
        action="store_true",
        help="Test multiple buffer sizes from 2^4 (16) to 2^12 (4096)",
    )
    parser.add_argument(
        "--compare-async",
        "-a",
        action="store_true",
        help="Compare disable_async=True vs False",
    )
    parser.add_argument(
        "--compare-unique",
        "-u",
        action="store_true",
        help="Compare only_insert_unique=True vs False",
    )

    args = parser.parse_args()

    config = BenchmarkConfig(
        buffer_size=args.buffer_size,
        num_operations=args.operations,
        warmup_operations=args.warmup,
    )

    if args.multi_size:
        if args.compare_async and args.compare_unique:
            for disable_async in [True, False]:
                for only_unique in [False, True]:
                    run_multi_size(config, disable_async=disable_async, only_insert_unique=only_unique)
                    print("\n" * 2)
        elif args.compare_async:
            run_multi_size(config, disable_async=True, only_insert_unique=False)
            print("\n" * 2)
            run_multi_size(config, disable_async=False, only_insert_unique=False)
        elif args.compare_unique:
            run_multi_size(config, disable_async=True, only_insert_unique=False)
            print("\n" * 2)
            run_multi_size(config, disable_async=True, only_insert_unique=True)
        else:
            run_multi_size(config, disable_async=True, only_insert_unique=False)
    elif args.compare_async or args.compare_unique:
        for disable_async in ([True, False] if args.compare_async else [True]):
            for only_unique in ([False, True] if args.compare_unique else [False]):
                config.disable_async = disable_async
                config.only_insert_unique = only_unique
                benchmark = BytesRingBufferBenchmark(config)
                benchmark.run()
                print("\n" * 2)
    else:
        benchmark = BytesRingBufferBenchmark(config)
        benchmark.run()


if __name__ == "__main__":
    main()
